{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at whether we can predict the number of bicycle trips across Seattle's Fremont Bridge based on weather, season, and other factors. We have a dataset with number of bikers that crossed the bridge on a particular day and some accompanying data for that day. We will perform a simple linear regression to relate weather and other information to bicycle counts, in order to estimate how a change in any one of these parameters affects the number of riders on a given day."
      ],
      "metadata": {
        "id": "2YX_utHD_s3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download the dataset and have a look at it."
      ],
      "metadata": {
        "id": "bAhCNW61AOWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "h_zsvUmV-RGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the data\n",
        "!gdown 12OEFoq_65x6Sy4doc24FwHu1ujKpbXSn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7frjiXeo8p7M",
        "outputId": "24c3b5de-72f5-492f-b057-14175b0d1809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12OEFoq_65x6Sy4doc24FwHu1ujKpbXSn\n",
            "To: /content/bikers_data.csv\n",
            "\r  0% 0.00/213k [00:00<?, ?B/s]\r100% 213k/213k [00:00<00:00, 89.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examine the columns in the dataset."
      ],
      "metadata": {
        "id": "1jH8ceoxAWWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the data\n",
        "data = pd.read_csv(\"bikers_data.csv\", index_col=\"Date\")\n",
        "\n",
        "#Features columns\n",
        "data_x_1 = data[['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'holiday', 'daylight_hrs', 'Rainfall (in)', 'dry day']].to_numpy()\n",
        "\n",
        "#Target column\n",
        "data_y_1 = data[[\"Number of bikers\"]].to_numpy()"
      ],
      "metadata": {
        "id": "5xykcINg-Evc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) Split the dataset into training and testing sets\n",
        "\n",
        "Split the dataset into training and testing sets. Keep 80% of the data for training and 20% of the data for testing."
      ],
      "metadata": {
        "id": "OZ6wbmQXAhzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an array of indices\n",
        "n_1 = data_x_1.shape[0]\n",
        "ind_1 = np.random.permutation(np.arange(0, n_1))\n",
        "\n",
        "#Extracting subarrays of corresponding indices\n",
        "n_train_1 = int(np.floor(0.8*n_1))\n",
        "ind_train_1 = ind_1[:n_train_1]\n",
        "n_test_1 = n_1-n_train_1\n",
        "ind_test_1 = ind_1[n_train_1:]\n",
        "\n",
        "#Normalization\n",
        "#x_train_max_1 = np.max(data_x_1[ind_train_1,:],axis=0)\n",
        "#x_train_min_1 = np.min(data_x_1[ind_train_1,:],axis=0)\n",
        "#data_x_1 = (data_x_1 - x_train_min_1)/(x_train_max_1 - x_train_min_1)\n",
        "\n",
        "#Splitting the dataset into train and test datasets\n",
        "data_x_train_1 = np.hstack((np.ones((n_train_1, 1)), data_x_1[ind_train_1,:]))\n",
        "data_y_train_1 = data_y_1[ind_train_1,:]\n",
        "data_x_test_1 = np.hstack((np.ones((n_test_1, 1)), data_x_1[ind_test_1,:]))\n",
        "data_y_test_1 = data_y_1[ind_test_1,:]"
      ],
      "metadata": {
        "id": "r5_C-VWea3iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Train a linear regression model\n",
        "\n",
        "Build a linear regression model for predicting the numner of bikers using the mean squared error loss function."
      ],
      "metadata": {
        "id": "iqVMAnc1AnjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using the closed-form solution of linear regression\n",
        "theta_1 = np.linalg.solve(data_x_train_1.T @ data_x_train_1, data_x_train_1.T @ data_y_train_1)\n",
        "\n",
        "#Using built-in function\n",
        "#sol_1 = np.linalg.lstsq(data_x_train_1, data_y_train_1, rcond=None)"
      ],
      "metadata": {
        "id": "BGFl2FVVeyA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c) Predict and Evaluate\n",
        "\n",
        "Predict on the test set and calculate the average absolute error between predictions and true value.\n",
        "\n",
        "Comment on the results."
      ],
      "metadata": {
        "id": "uiMkCZAnAy-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the average absolute error\n",
        "alpha_1 = np.sum(np.absolute(data_y_test_1 - data_x_test_1@theta_1))/n_test_1\n",
        "print(alpha_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxjJsxe_fgmN",
        "outputId": "961c6a28-2638-47f4-9fa8-a1b625912835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1886.98504783931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error is of the 1e^3 order of magnitude which is useable (not ideal however), since our measurements for number of bikers is of greater than or equal order of magnitude."
      ],
      "metadata": {
        "id": "tofX26RgjnKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d) What is the expected number of bikers on a dry non-holiday Monday with 0 inches of rain and 10.5 hours of daylight?"
      ],
      "metadata": {
        "id": "Gqu61LeEBKMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inputting the requested values of variables into our approximation\n",
        "print((np.array([1,1,0,0,0,0,0,0,0,10.5,0,0]) @ theta_1).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gHRUI6zgX6t",
        "outputId": "ddd86900-7446-497c-8b24-556d3b89cb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10764.606984479518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## e) Notice that the dataset had an another column 'Temp (F)' but we aren't using it. Let's use that too and do this again.\n",
        "\n",
        "Add the 'Temp (F)' column to our X data."
      ],
      "metadata": {
        "id": "cUYLZMSRC9B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Features columns\n",
        "data_x_2 = data[['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'holiday', 'daylight_hrs', 'Rainfall (in)', 'dry day', 'Temp (F)']].to_numpy()\n",
        "\n",
        "#Target column\n",
        "data_y_2 = data[[\"Number of bikers\"]].to_numpy()"
      ],
      "metadata": {
        "id": "3v9F2IGA-9Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## f) Split the dataset into training and testing sets again\n"
      ],
      "metadata": {
        "id": "OUGW1UulDVfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an array of indices\n",
        "n_2 = data_x_2.shape[0]\n",
        "ind_2 = np.random.permutation(np.arange(0, n_2))\n",
        "\n",
        "#Extracting subarrays of corresponding indices\n",
        "n_train_2 = int(np.floor(0.8*n_2))\n",
        "ind_train_2 = ind_2[:n_train_2]\n",
        "n_test_2 = n_2-n_train_2\n",
        "ind_test_2 = ind_2[n_train_2:]\n",
        "\n",
        "#Normalization\n",
        "#x_train_max_2 = np.max(data_x_2[ind_train_2,:],axis=0)\n",
        "#x_train_min_2 = np.min(data_x_2[ind_train_2,:],axis=0)\n",
        "#data_x_2 = (data_x_2 - x_train_min_2)/(x_train_max_2 - x_train_min_2)\n",
        "\n",
        "#Splitting the dataset into train and test datasets\n",
        "data_x_train_2 = np.hstack((np.ones((n_train_2, 1)), data_x_2[ind_train_2,:]))\n",
        "data_y_train_2 = data_y_2[ind_train_2,:]\n",
        "data_x_test_2 = np.hstack((np.ones((n_test_2, 1)), data_x_2[ind_test_2,:]))\n",
        "data_y_test_2 = data_y_2[ind_test_2,:]"
      ],
      "metadata": {
        "id": "geiYjkQ8kk17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## g) Train a linear regression model again\n",
        "\n",
        "Build a linear regression model for predicting the numner of bikers using the mean squared error loss function."
      ],
      "metadata": {
        "id": "cQEouVbUDbea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using the closed-form solution of linear regression\n",
        "theta_2 = np.linalg.solve(data_x_train_2.T @ data_x_train_2, data_x_train_2.T @ data_y_train_2)\n",
        "\n",
        "#Using built-in function\n",
        "#sol_2 = np.linalg.lstsq(data_x_train_2, data_y_train_2, rcond=None)"
      ],
      "metadata": {
        "id": "R7x6y203klV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## h) Predict and Evaluate Again!\n",
        "\n",
        "Predict on the test set and calculate the average absolute error between predictions and true value.\n",
        "\n",
        "Comment on the results. Did it change? Can you think of any information we can use to make a better(more informed) model?"
      ],
      "metadata": {
        "id": "FqIsCLNcDhwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the average absolute error\n",
        "alpha_2 = np.sum(np.absolute(data_y_test_2 - data_x_test_2@theta_2))/n_test_2\n",
        "print(alpha_2)"
      ],
      "metadata": {
        "id": "h2S7Z-0c3WFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4d1d67-40e6-4d90-9153-10faceb2868a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1772.024778891665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see the new error is slightly smaller which makes sense, since we added one more column of (intuitively) high-correlation data.\n",
        "\n",
        "In order to improve the model further we should consider:\n",
        "\n",
        "1) Removing redundancy. For example, \"dry day\" and \"Rainfaill (in)\" columns are extremely similar (the data of the first is implied by the data of the second). By considering both of these values we are taking \"attention\" away from more important information.\n",
        "\n",
        "2) Utilize more continuous data. Nearly all of our columns are binary, which increases the error, since linear regression outputs a continuous function. If we had more than two data points for some columns, we would get a more accurate model. For example, the \"Rainfall (in)\" column is a continuous generalization of the \"dry day\" column (we should aim to do something similar with other ones as well).\n",
        "\n",
        "3) Incorporate more high-correlation data. When we started observing temperature, our error decreased because there is a correlation between that and persons willingness to bike. We should attempt to add more information similar to these (e.g. more climate measurements (wind speed, cloud cover, etc.), some coefficient of the current quality of bike lanes (e.g. if there is a new bike lane constructed, the coefficient increases; if vandals destroy part of the line, the coefficient decreases), etc.)"
      ],
      "metadata": {
        "id": "VHPbUokM_LJF"
      }
    }
  ]
}
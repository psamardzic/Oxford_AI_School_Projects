{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpkyHj1X9aBL"
      },
      "source": [
        "# Q1. Predicting housing prices\n",
        "\n",
        "In this problem, you will generate models for predicting house prices from given house features. The file “house_prices.txt” contains the data.\n",
        "\n",
        "There are four features used in this regression:\n",
        "\n",
        "* one binary feature (whether or not the house has covered parking)\n",
        "* one numerical feature (size, measure in thousands of square meters)\n",
        "* two categorical features (architectural style and location)\n",
        "\n",
        "Each of the categorial features is represented as a vector of size 3. This gives us feature vectors of size 8 in total for each house. 200 examples are given in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bieYZZvY9aBO"
      },
      "outputs": [],
      "source": [
        "#Importing libraries\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim, utils\n",
        "from sklearn import metrics\n",
        "#import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting the device to the GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "#Training function\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    for X, y in dataloader:\n",
        "        yhat = model(X)\n",
        "        loss = loss_fn(yhat, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "#Test function (loss)\n",
        "def test_loss(features, target, model, loss_fn):\n",
        "    with torch.no_grad():\n",
        "        yhat = model(features)\n",
        "        loss = loss_fn(yhat, target).item()\n",
        "        return round(loss, 6)\n",
        "\n",
        "#Test function (accuracy)\n",
        "def test_accuracy(features, target, model):\n",
        "    with torch.no_grad():\n",
        "        yhat = model(features)\n",
        "\n",
        "        accuracy = torch.sum(torch.eq(torch.argmax(yhat, dim=1), target))/target.size(dim=0)\n",
        "        accuracy = accuracy.item()*100\n",
        "        return f\"{round(accuracy, 4)}%\""
      ],
      "metadata": {
        "id": "YOEQS7IYiw7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOV1zQntRzX-",
        "outputId": "a0ee8402-c2e3-4f78-c1bb-ef708e7dee4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hLxbWVrnmSnZDGjvu8HeQex6ezbXeLB6\n",
            "To: /content/house_prices.txt\n",
            "\r  0%|          | 0.00/6.08k [00:00<?, ?B/s]\r100%|██████████| 6.08k/6.08k [00:00<00:00, 20.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "#Downloading the data\n",
        "%%bash\n",
        "gdown 1hLxbWVrnmSnZDGjvu8HeQex6ezbXeLB6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6wzKRjO-dDQ"
      },
      "outputs": [],
      "source": [
        "#Creating a path to our data\n",
        "DATA_DIR = pathlib.Path(\"/content\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8MAlhME9aBQ"
      },
      "outputs": [],
      "source": [
        "#Loading the data\n",
        "features_1 = pd.read_csv(\n",
        "    DATA_DIR / \"house_prices.txt\",\n",
        "    delimiter=\"\\t\",\n",
        "    skiprows=[0,1,2,3,4],\n",
        "    header=None,\n",
        "    usecols=[0,1,2,3,4,6,7,8],\n",
        "    names=[\"parking\", \"sq_meters\", \"art_deco\", \"bungalow\", \"colonial\", \"west\", \"east\", \"north\"],\n",
        "    dtype=np.float32\n",
        ")\n",
        "\n",
        "target_1 = pd.read_csv(\n",
        "    DATA_DIR / \"house_prices.txt\",\n",
        "    delimiter=\"\\t\",\n",
        "    skiprows=[0,1,2,3,4],\n",
        "    header=None,\n",
        "    usecols=[5],\n",
        "    names=[\"price\"],\n",
        "    dtype=np.float32\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVncPwnc9aBS"
      },
      "source": [
        "## a) Split the dataset into training and testing sets\n",
        "\n",
        "Split the dataset into training and testing sets. Keep 80% of the data for training and 20% of the data for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20JaHLbQ9aBT"
      },
      "outputs": [],
      "source": [
        "#Creating an array of indices\n",
        "n_1 = features_1.shape[0]\n",
        "ind_1 = np.random.permutation(np.arange(0, n_1))\n",
        "\n",
        "#Extracting subarrays of corresponding indices\n",
        "n_train_1 = int(np.floor(0.8*n_1))\n",
        "ind_train_1 = ind_1[:n_train_1]\n",
        "n_test_1 = n_1-n_train_1\n",
        "ind_test_1 = ind_1[n_train_1:]\n",
        "\n",
        "#Converting our data to numpy arrays\n",
        "features_np_1 = features_1.to_numpy()\n",
        "target_np_1 = target_1.to_numpy()\n",
        "\n",
        "#Splitting the dataset into train and test datasets\n",
        "features_train_1 = features_np_1[ind_train_1,:]\n",
        "target_train_1 = target_np_1[ind_train_1,:]\n",
        "features_test_1 = features_np_1[ind_test_1,:]\n",
        "target_test_1 = target_np_1[ind_test_1,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc7Z2CET9aBV"
      },
      "source": [
        "## b) Build a neural network to predict house prices\n",
        "\n",
        "Use the provided dataloaders, define a neural network with two hidden layers of 10 units each both with ReLU activation. The output unit should have no activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcU1RHH89aBV"
      },
      "outputs": [],
      "source": [
        "#Creating datasets\n",
        "features_train_tensor_1 = torch.from_numpy(features_train_1).to(device)\n",
        "target_train_tensor_1 = torch.from_numpy(target_train_1).to(device)\n",
        "dataset_train_1 = utils.data.TensorDataset(features_train_tensor_1, target_train_tensor_1)\n",
        "\n",
        "features_test_tensor_1 = torch.from_numpy(features_test_1).to(device)\n",
        "target_test_tensor_1 = torch.from_numpy(target_test_1).to(device)\n",
        "dataset_test_1 = utils.data.TensorDataset(features_test_tensor_1, target_test_tensor_1)\n",
        "\n",
        "#Creating dataloader\n",
        "dataloader_train_1 = utils.data.DataLoader(dataset_train_1, batch_size=40, shuffle=True)\n",
        "\n",
        "dataloader_test_1 = utils.data.DataLoader(dataset_test_1, batch_size=40, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLfoSrWd9aBW"
      },
      "source": [
        "Define a neural network with two hidden layers of 10 units each both with ReLU activation. The output unit should have no activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClxvctlZ9aBW"
      },
      "outputs": [],
      "source": [
        "#Defining our neural network\n",
        "class Net_1(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net_1, self).__init__()\n",
        "        self.lin1 = nn.Linear(8, 10)\n",
        "        self.activ1 = nn.ReLU()\n",
        "        self.lin2 = nn.Linear(10, 10)\n",
        "        self.activ2 = nn.ReLU()\n",
        "        self.out = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = self.activ1(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.activ2(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRKC6VPw9aBW"
      },
      "source": [
        "## c) Train your neural network using gradient descent\n",
        "\n",
        "Using mean square loss, train the network with an appropriate optimizer for a few hundred epochs and plot the loss versus the number of training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbsEpRls9aBW"
      },
      "outputs": [],
      "source": [
        "#Defining an instance of our network and other parameters required for training\n",
        "net_1 = Net_1().to(device)\n",
        "loss_fn_1 = nn.MSELoss()\n",
        "optimizer_1 = optim.Adam(net_1.parameters(), lr=1e-3)\n",
        "\n",
        "#Training our network\n",
        "epochs_1 = 200\n",
        "for epoch_1 in range(epochs_1):\n",
        "    train(dataloader_train_1, net_1, loss_fn_1, optimizer_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOZPKa8h9aBX"
      },
      "source": [
        "## d) Assess the performance of your neural network\n",
        "\n",
        "Assess the performance of your neural network on the training data and the testing data. Comment on the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUqGwlQX9aBY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2853e831-9b34-496f-e027-3b8f6b57b8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.002593\n",
            "0.003833\n"
          ]
        }
      ],
      "source": [
        "#Calculating loss of our network on the train and test datasets\n",
        "print(test_loss(features_train_tensor_1, target_train_tensor_1, net_1, loss_fn_1))\n",
        "print(test_loss(features_test_tensor_1, target_test_tensor_1, net_1, loss_fn_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwK7at_iq3ex"
      },
      "source": [
        "The predictions seem to be good enough, as the errors are of the order of magnitude $10^{-3}$, but the true values are in the $10^{-1}$ range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywMbqKP2AORQ"
      },
      "source": [
        "# Q2. Classification\n",
        "\n",
        "Consider the following simulated classification dataset with N = 300 samples labeled with one of three class labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RCsg-mtR20C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ff8e88-1a90-497e-8a0e-01f394ccfa83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_n5odu1KSpBAqO_Ddbqy2Ty0mG12Dr0A\n",
            "To: /content/spiral_features.npy\n",
            "\r  0%|          | 0.00/4.93k [00:00<?, ?B/s]\r100%|██████████| 4.93k/4.93k [00:00<00:00, 18.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1C7GK_uY0srlUdLfHfGa4IZcfthmmdROr\n",
            "To: /content/spiral_target.npy\n",
            "\r  0%|          | 0.00/428 [00:00<?, ?B/s]\r100%|██████████| 428/428 [00:00<00:00, 2.13MB/s]\n"
          ]
        }
      ],
      "source": [
        "#Downloading the data\n",
        "%%bash\n",
        "gdown 1_n5odu1KSpBAqO_Ddbqy2Ty0mG12Dr0A\n",
        "gdown 1C7GK_uY0srlUdLfHfGa4IZcfthmmdROr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g1OV8ho9aBZ"
      },
      "outputs": [],
      "source": [
        "#Loading the data\n",
        "features_2 = np.load(DATA_DIR / \"spiral_features.npy\").astype(np.float32)\n",
        "target_2 = np.load(DATA_DIR / \"spiral_target.npy\")\n",
        "\n",
        "#Visualizing the data\n",
        "#plt.scatter(features_2[:, 0], features_2[:, 1], c=target_2, s=20)\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef_iVNRoAZp4"
      },
      "source": [
        "## a) Split the dataset into training and testing sets\n",
        "\n",
        "Split the dataset into training and testing sets. Keep 80% of the data for training and 20% of the data for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE4YSz1CAWCQ"
      },
      "outputs": [],
      "source": [
        "#Creating an array of indices\n",
        "n_2 = features_2.shape[0]\n",
        "ind_2 = np.random.permutation(np.arange(0, n_2))\n",
        "\n",
        "#Extracting subarrays of corresponding indices\n",
        "n_train_2 = int(np.floor(0.8*n_2))\n",
        "ind_train_2 = ind_2[:n_train_2]\n",
        "n_test_2 = n_2-n_train_2\n",
        "ind_test_2 = ind_2[n_train_2:]\n",
        "\n",
        "#Splitting the dataset into train and test datasets\n",
        "features_train_2 = features_2[ind_train_2,:]\n",
        "target_train_2 = target_2[ind_train_2]\n",
        "features_test_2 = features_2[ind_test_2,:]\n",
        "target_test_2 = target_2[ind_test_2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpZJHaKtAcw6"
      },
      "source": [
        "## b) Train a logistic regression model\n",
        "\n",
        "Using the provided datasets and dataloaders, build a 3-class logistic regression classifier for this dataset using PyTorch and train it for 2000 epochs using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr7zrbEfAbKq"
      },
      "outputs": [],
      "source": [
        "#Creating datasets\n",
        "features_train_tensor_2 = torch.from_numpy(features_train_2).to(device)\n",
        "target_train_tensor_2 = torch.from_numpy(target_train_2).to(device)\n",
        "dataset_train_2 = utils.data.TensorDataset(features_train_tensor_2, target_train_tensor_2)\n",
        "\n",
        "features_test_tensor_2 = torch.from_numpy(features_test_2).to(device)\n",
        "target_test_tensor_2 = torch.from_numpy(target_test_2).to(device)\n",
        "dataset_test_2 = utils.data.TensorDataset(features_test_tensor_2, target_test_tensor_2)\n",
        "\n",
        "#Creating dataloaders\n",
        "dataloader_train_2 = utils.data.DataLoader(dataset_train_2, batch_size=40, shuffle=True)\n",
        "\n",
        "dataloader_test_2 = utils.data.DataLoader(dataset_test_2, batch_size=40, shuffle=False)\n",
        "\n",
        "#Adding \"higher degree columns\" to our logistic regression dataset so our predictions become more accurate\n",
        "deg_logreg_2 = 4\n",
        "\n",
        "features_train_tensor_logreg_2 = torch.empty((features_train_tensor_2.size(dim=0),0)).to(device)\n",
        "for i in range(deg_logreg_2):\n",
        "    for j in range(deg_logreg_2):\n",
        "        features_train_tensor_logreg_2 = torch.cat((features_train_tensor_logreg_2, (features_train_tensor_2[:,0]**i * features_train_tensor_2[:,1]**j).reshape(-1,1)), 1)\n",
        "dataset_train_logreg_2 = utils.data.TensorDataset(features_train_tensor_logreg_2, target_train_tensor_2)\n",
        "dataloader_train_logreg_2 = utils.data.DataLoader(dataset_train_logreg_2, batch_size=40, shuffle=True)\n",
        "\n",
        "features_test_tensor_logreg_2 = torch.empty((features_test_tensor_2.size(dim=0),0)).to(device)\n",
        "for i in range(deg_logreg_2):\n",
        "    for j in range(deg_logreg_2):\n",
        "        features_test_tensor_logreg_2 = torch.cat((features_test_tensor_logreg_2, (features_test_tensor_2[:,0]**i * features_test_tensor_2[:,1]**j).reshape(-1,1)), 1)\n",
        "dataset_test_logreg_2 = utils.data.TensorDataset(features_test_tensor_logreg_2, target_test_tensor_2)\n",
        "dataloader_test_logreg_2 = utils.data.DataLoader(dataset_test_logreg_2, batch_size=40, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGurrQy6A04S"
      },
      "outputs": [],
      "source": [
        "#Defining our logistic regression module\n",
        "class LogReg_2(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LogReg_2, self).__init__()\n",
        "        self.out = nn.Linear(16, 3)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.out(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqouOaI_l4Sa"
      },
      "outputs": [],
      "source": [
        "#Defining an instance of our logistic regression module and other parameters required for training\n",
        "logreg_2 = LogReg_2().to(device)\n",
        "loss_fn_logreg_2 = nn.CrossEntropyLoss()\n",
        "optimizer_logreg_2 = optim.Adam(logreg_2.parameters(), lr=1e-3)\n",
        "\n",
        "#Training our network\n",
        "epochs_logreg_2 = 2000\n",
        "for epoch_logreg_2 in range(epochs_logreg_2):\n",
        "    train(dataloader_train_logreg_2, logreg_2, loss_fn_logreg_2, optimizer_logreg_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKeAu9DxBtIQ"
      },
      "source": [
        "## c) Assess the performance of your logistic regression model\n",
        "\n",
        "Assess the performance of your model on the training data and the testing data in terms of both loss and accuracy. Comment on the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxI8YTTABqX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8ed28b-5739-4e6a-9226-5b0a9626a91f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.681968\n",
            "90.4167%\n",
            "\n",
            "0.703065\n",
            "86.6667%\n"
          ]
        }
      ],
      "source": [
        "#Calculating loss and accuracy of our network on the train and test datasets\n",
        "print(test_loss(features_train_tensor_logreg_2, target_train_tensor_2, logreg_2, loss_fn_logreg_2))\n",
        "print(test_accuracy(features_train_tensor_logreg_2, target_train_tensor_2, logreg_2))\n",
        "\n",
        "print()\n",
        "\n",
        "print(test_loss(features_test_tensor_logreg_2, target_test_tensor_2, logreg_2, loss_fn_logreg_2))\n",
        "print(test_accuracy(features_test_tensor_logreg_2, target_test_tensor_2, logreg_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKroZVuEtQhU"
      },
      "source": [
        "The results are pretty good, however, there is still some room for improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVE3pwIoB6lk"
      },
      "source": [
        "## d) Build a neural network classifier\n",
        "\n",
        "Build a 3-class classifier using a neural network with one hidden layer of 100 units and a ReLU activation. Train the network with a gradient descent algorithm and for 2000 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN0IhgvlB3Ox"
      },
      "outputs": [],
      "source": [
        "#Defining our neural network\n",
        "class Net_2(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net_2, self).__init__()\n",
        "        self.lin = nn.Linear(2, 100)\n",
        "        self.activ = nn.ReLU()\n",
        "        self.out = nn.Linear(100, 3)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin(x)\n",
        "        x = self.activ(x)\n",
        "        x = self.out(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDuHInV_whOL"
      },
      "outputs": [],
      "source": [
        "#Defining an instance of our network and other parameters required for training\n",
        "net_2 = Net_2().to(device)\n",
        "loss_fn_2 = nn.CrossEntropyLoss()\n",
        "optimizer_2 = optim.Adam(net_2.parameters(), lr=1e-3)\n",
        "\n",
        "#Training our network\n",
        "epochs_2 = 2000\n",
        "for epoch_2 in range(epochs_2):\n",
        "    train(dataloader_train_2, net_2, loss_fn_2, optimizer_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrqkw0XgCNDY"
      },
      "source": [
        "## e) Assess the performance of your neural network\n",
        "\n",
        "Assess the performance of your neural network on the training data and the testing data in terms of both loss and accuracy. Comment on the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5z5M2-aCKZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39048465-f36a-47e7-9aa9-1f8b66089a58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.560955\n",
            "99.1667%\n",
            "\n",
            "0.551863\n",
            "100.0%\n"
          ]
        }
      ],
      "source": [
        "#Calculating loss and accuracy of our network on the train and test datasets\n",
        "print(test_loss(features_train_tensor_2, target_train_tensor_2, net_2, loss_fn_2))\n",
        "print(test_accuracy(features_train_tensor_2, target_train_tensor_2, net_2))\n",
        "\n",
        "print()\n",
        "\n",
        "print(test_loss(features_test_tensor_2, target_test_tensor_2, net_2, loss_fn_2))\n",
        "print(test_accuracy(features_test_tensor_2, target_test_tensor_2, net_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MiuxHw5rQt8"
      },
      "source": [
        "The error and the accuracy are nearly the same on both datasets and the accuracy is close to perfect. These things imply that we did a good job of constructing a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2bSpQJiCa8v"
      },
      "source": [
        "## f) Interpret the confusion matrix\n",
        "\n",
        "Compute the confusion matrix for your classifier on both the training and testing data and interpret the results. You may either code your own confusion matrix or use the following function from Scikit-Learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3Qfs6IOCs-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a53a153-2add-43be-ebf8-505c78c8223b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[85  1  0]\n",
            " [ 0 78  0]\n",
            " [ 0  1 75]]\n",
            "\n",
            "[[14  0  0]\n",
            " [ 0 22  0]\n",
            " [ 0  0 24]]\n"
          ]
        }
      ],
      "source": [
        "#Printing confusion matrices using the built-in function\n",
        "with torch.no_grad():\n",
        "    yhat_train_2 = net_2(features_train_tensor_2)\n",
        "    yhat_test_2 = net_2(features_test_tensor_2)\n",
        "\n",
        "    print(metrics.confusion_matrix(target_train_tensor_2.cpu(), torch.argmax(yhat_train_2, dim=1).cpu()))\n",
        "    print()\n",
        "    print(metrics.confusion_matrix(target_test_tensor_2.cpu(), torch.argmax(yhat_test_2, dim=1).cpu()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH4NsmwvftHR"
      },
      "source": [
        "Since everything outside the main diagonal is zero or close to zero, that means that are network is very successful in doing its job of classifying the points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c42vE4YnD3sD"
      },
      "source": [
        "## g) Create your own deep neural network!\n",
        "\n",
        "Create your own deep neural network and try to get as high an accuracy score on the test set as possible. Be creative: this is an opportunity for you to demonstrate what you can do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzLkqMWJBBRj"
      },
      "outputs": [],
      "source": [
        "#Creating an array of indices\n",
        "n_2b = features_2.shape[0]\n",
        "ind_2b = np.random.permutation(np.arange(0, n_2b))\n",
        "\n",
        "#Extracting subarrays of corresponding indices\n",
        "n_train_2b = int(np.floor(0.667*n_2b))\n",
        "ind_train_2b = ind_2b[:n_train_2b]\n",
        "n_validation_2b = int(np.floor(0.167*n_2b))\n",
        "ind_validation_2b = ind_2b[n_train_2b:n_train_2b+n_validation_2b]\n",
        "n_test_2b = n_2b-n_train_2b-n_validation_2b\n",
        "ind_test_2b = ind_2b[n_train_2b+n_validation_2b:]\n",
        "\n",
        "\n",
        "#Splitting the dataset into train, validation and test datasets\n",
        "features_train_2b = features_2[ind_train_2b,:]\n",
        "target_train_2b = target_2[ind_train_2b]\n",
        "features_validation_2b = features_2[ind_validation_2b,:]\n",
        "target_validation_2b = target_2[ind_validation_2b]\n",
        "features_test_2b = features_2[ind_test_2b,:]\n",
        "target_test_2b = target_2[ind_test_2b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r8kT0kyCrjx"
      },
      "outputs": [],
      "source": [
        "#Creating datasets\n",
        "features_train_tensor_2b = torch.from_numpy(features_train_2b).to(device)\n",
        "target_train_tensor_2b = torch.from_numpy(target_train_2b).to(device)\n",
        "dataset_train_2b = utils.data.TensorDataset(features_train_tensor_2b, target_train_tensor_2b)\n",
        "\n",
        "features_validation_tensor_2b = torch.from_numpy(features_validation_2b).to(device)\n",
        "target_validation_tensor_2b = torch.from_numpy(target_validation_2b).to(device)\n",
        "dataset_validation_2b = utils.data.TensorDataset(features_validation_tensor_2b, target_validation_tensor_2b)\n",
        "\n",
        "features_test_tensor_2b = torch.from_numpy(features_test_2b).to(device)\n",
        "target_test_tensor_2b = torch.from_numpy(target_test_2b).to(device)\n",
        "dataset_test_2b = utils.data.TensorDataset(features_test_tensor_2b, target_test_tensor_2b)\n",
        "\n",
        "#Creating dataloaders\n",
        "dataloader_train_2b = utils.data.DataLoader(dataset_train_2b, batch_size=25, shuffle=True)\n",
        "\n",
        "dataloader_validation_2b = utils.data.DataLoader(dataset_validation_2b, batch_size=25, shuffle=False)\n",
        "\n",
        "dataloader_test_2b = utils.data.DataLoader(dataset_test_2b, batch_size=25, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3o5DXN0IJnc"
      },
      "outputs": [],
      "source": [
        "#Defining my custom neural network with 2 hidden layers (100 units and 80 units)\n",
        "class Net_2b(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net_2b, self).__init__()\n",
        "        self.lin1 = nn.Linear(2, 100)\n",
        "        self.activ1 = nn.ReLU()\n",
        "        self.lin2 = nn.Linear(100, 80)\n",
        "        self.activ2 = nn.ReLU()\n",
        "        self.out = nn.Linear(80, 3)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = self.activ1(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.activ2(x)\n",
        "        x = self.out(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p717T6Nq666h"
      },
      "outputs": [],
      "source": [
        "#Defining an instance of my network and other parameters required for training\n",
        "net_2b = Net_2b().to(device)\n",
        "loss_fn_2b = nn.CrossEntropyLoss()\n",
        "optimizer_2b = optim.Adam(net_2b.parameters(), lr=1e-4)\n",
        "\n",
        "#Training our network\n",
        "epochs_2b = 1000\n",
        "for epoch_2b in range(epochs_2b):\n",
        "    train(dataloader_train_2b, net_2b, loss_fn_2b, optimizer_2b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la-i7gu28rdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "207d23ac-53f9-457d-b749-a2855946f742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.587577\n",
            "98.5%\n",
            "\n",
            "0.558024\n",
            "100.0%\n"
          ]
        }
      ],
      "source": [
        "#Calculating loss and accuracy of our network on the train and validation datasets\n",
        "print(test_loss(features_train_tensor_2b, target_train_tensor_2b, net_2b, loss_fn_2b))\n",
        "print(test_accuracy(features_train_tensor_2b, target_train_tensor_2b, net_2b))\n",
        "\n",
        "print()\n",
        "\n",
        "print(test_loss(features_validation_tensor_2b, target_validation_tensor_2b, net_2b, loss_fn_2b))\n",
        "print(test_accuracy(features_validation_tensor_2b, target_validation_tensor_2b, net_2b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2YTzCSPEhng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d168667-aac0-4874-dcb0-951e58295e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.571689\n",
            "100.0%\n"
          ]
        }
      ],
      "source": [
        "#Calculating loss and accuracy of our network on the test dataset\n",
        "print(test_loss(features_test_tensor_2b, target_test_tensor_2b, net_2b, loss_fn_2b))\n",
        "print(test_accuracy(features_test_tensor_2b, target_test_tensor_2b, net_2b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlGf3M3_eD8v"
      },
      "source": [
        "I am consistently getting a $100\\%$ success rate on the test dataset, which means that our network is working great (even better than the first one). However, we are limited because we only have $300$ input datapoints, so there might be an even better neural network that we could only find if we had more data.\n",
        "\n",
        "To find the optimal hyperparameters, I varied the number of layers, number of units per layer, the learning rate, number of total epochs, and I ended up with this model. I deleted the loops which helped me find the optimal values more efficiently, because we no longer need them."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}